{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "26037d32-2047-4157-81ef-595916bd66a0"
            },
            "source": [
                "# Checkpoint Three: Cleaning Data\n",
                "\n",
                "Now you are ready to clean your data. Before starting coding, provide the link to your dataset below.\n",
                "\n",
                "My dataset:\n",
                "\n",
                "Import the necessary libraries and create your dataframe(s)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "e8adef8e-d0f2-4640-a179-5997f11e82ca"
            },
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>company</th>\n",
                            "      <th>bean_origin</th>\n",
                            "      <th>ref</th>\n",
                            "      <th>review_date</th>\n",
                            "      <th>cocoa_percent</th>\n",
                            "      <th>company_location</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>bean_type</th>\n",
                            "      <th>broad_bean_origin</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>A. Morin</td>\n",
                            "      <td>Agua Grande</td>\n",
                            "      <td>1876</td>\n",
                            "      <td>2016</td>\n",
                            "      <td>63.0</td>\n",
                            "      <td>France</td>\n",
                            "      <td>3.75</td>\n",
                            "      <td></td>\n",
                            "      <td>Sao Tome</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>A. Morin</td>\n",
                            "      <td>Kpime</td>\n",
                            "      <td>1676</td>\n",
                            "      <td>2015</td>\n",
                            "      <td>70.0</td>\n",
                            "      <td>France</td>\n",
                            "      <td>2.75</td>\n",
                            "      <td></td>\n",
                            "      <td>Togo</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>A. Morin</td>\n",
                            "      <td>Atsane</td>\n",
                            "      <td>1676</td>\n",
                            "      <td>2015</td>\n",
                            "      <td>70.0</td>\n",
                            "      <td>France</td>\n",
                            "      <td>3.00</td>\n",
                            "      <td></td>\n",
                            "      <td>Togo</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>A. Morin</td>\n",
                            "      <td>Akata</td>\n",
                            "      <td>1680</td>\n",
                            "      <td>2015</td>\n",
                            "      <td>70.0</td>\n",
                            "      <td>France</td>\n",
                            "      <td>3.50</td>\n",
                            "      <td></td>\n",
                            "      <td>Togo</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>A. Morin</td>\n",
                            "      <td>Quilla</td>\n",
                            "      <td>1704</td>\n",
                            "      <td>2015</td>\n",
                            "      <td>70.0</td>\n",
                            "      <td>France</td>\n",
                            "      <td>3.50</td>\n",
                            "      <td></td>\n",
                            "      <td>Peru</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "    company  bean_origin   ref  review_date  cocoa_percent company_location  \\\n",
                            "0  A. Morin  Agua Grande  1876         2016           63.0           France   \n",
                            "1  A. Morin        Kpime  1676         2015           70.0           France   \n",
                            "2  A. Morin       Atsane  1676         2015           70.0           France   \n",
                            "3  A. Morin        Akata  1680         2015           70.0           France   \n",
                            "4  A. Morin       Quilla  1704         2015           70.0           France   \n",
                            "\n",
                            "   rating bean_type broad_bean_origin  \n",
                            "0    3.75                    Sao Tome  \n",
                            "1    2.75                        Togo  \n",
                            "2    3.00                        Togo  \n",
                            "3    3.50                        Togo  \n",
                            "4    3.50                        Peru  "
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# I'm importing pandas and numpy to help with the data cleanup.\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Now that I've moved the file into this folder,I can load it directly.\n",
                "df = pd.read_csv('flavors_of_cacao.csv')\n",
                "\n",
                "# The column names are really messy,so I'm renaming them to simple words.\n",
                "# This makes it much easier to write the rest of my cleaning code.\n",
                "df.columns = ['company', 'bean_origin', 'ref', 'review_date', 'cocoa_percent', \n",
                "              'company_location', 'rating', 'bean_type', 'broad_bean_origin']\n",
                "\n",
                "# Turning the cocoa percentage into a number so I can do math with it later.\n",
                "df['cocoa_percent'] = df['cocoa_percent'].str.replace('%', '').astype(float)\n",
                "\n",
                "# Just checking the first few rows to make sure it loaded right.\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "e172475a-c4ee-414a-8367-9965355dbba6"
            },
            "source": [
                "## Missing Data\n",
                "\n",
                "Test your dataset for missing data and handle it as needed. Make notes in the form of code comments as to your thought process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "azdata_cell_guid": "e1dc66ef-e471-4c27-92e7-ee878c106eba"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "company              0\n",
                        "bean_origin          0\n",
                        "ref                  0\n",
                        "review_date          0\n",
                        "cocoa_percent        0\n",
                        "company_location     0\n",
                        "rating               0\n",
                        "bean_type            1\n",
                        "broad_bean_origin    1\n",
                        "dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "# First, I'll check for any obvious null values using isnull.\n",
                "print(df.isnull().sum())\n",
                "\n",
                "# My thought process: I noticed that even where it says 0 nulls for some columns, \n",
                "# there are actually empty spaces or blanks in the 'bean_type' and 'broad_bean_origin' columns. \n",
                "# I don't want to delete these rows because I'll lose too much data. \n",
                "# Instead, I'm filling those empty spots with 'Unknown' so the data is consistent.\n",
                "\n",
                "df['bean_type'] = df['bean_type'].fillna('Unknown')\n",
                "df['bean_type'] = df['bean_type'].apply(lambda x: 'Unknown' if str(x).strip() == \"\" else x)\n",
                "\n",
                "df['broad_bean_origin'] = df['broad_bean_origin'].fillna('Unknown')\n",
                "df['broad_bean_origin'] = df['broad_bean_origin'].apply(lambda x: 'Unknown' if str(x).strip() == \"\" else x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "1233f543-e9a0-4f78-96f5-d7536554102e"
            },
            "source": [
                "## Irregular Data\n",
                "\n",
                "Detect outliers in your dataset and handle them as needed. Use code comments to make notes about your thought process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "azdata_cell_guid": "efed50ae-16f0-471d-98e2-632553a74c12"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Number of outliers found: 19\n"
                    ]
                }
            ],
            "source": [
                "# I'm using the IQR (Interquartile Range) method to check for any ratings that are way outside the normal range.\n",
                "Q1 = df['rating'].quantile(0.25)\n",
                "Q3 = df['rating'].quantile(0.75)\n",
                "IQR = Q3 - Q1\n",
                "\n",
                "lower_bound = Q1 - 1.5 * IQR\n",
                "upper_bound = Q3 + 1.5 * IQR\n",
                "\n",
                "# This helps me see if there are any bars with really extreme ratings.\n",
                "outliers = df[(df['rating'] < lower_bound) | (df['rating'] > upper_bound)]\n",
                "print(f\"Number of outliers found: {len(outliers)}\")\n",
                "\n",
                "# My thought process: Even though these are technically \"outliers\" (like a 1.0 rating), \n",
                "# I'm deciding to keep them. In chocolate tasting, some bars are just really bad, \n",
                "# and that's important information for my analysis of quality."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "6f5b8ee0-bab3-44bc-958a-67d1e4c0407f"
            },
            "source": [
                "## Unnecessary Data\n",
                "\n",
                "Look for the different types of unnecessary data in your dataset and address it as needed. Make sure to use code comments to illustrate your thought process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "azdata_cell_guid": "e788a239-2fbf-41de-9bd3-19e52e3b187c"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Duplicates found: 0\n"
                    ]
                }
            ],
            "source": [
                "# I'm checking to see if there are any identical rows that got entered twice.\n",
                "duplicates = df.duplicated().sum()\n",
                "print(f\"Duplicates found: {duplicates}\")\n",
                "\n",
                "# My thought process: Since I didn't find any exact duplicates, I don't need to drop rows. \n",
                "# I'm also keeping the 'ref' and 'review_date' columns for now because they might be \n",
                "# useful if I want to see if chocolate quality has changed over the years."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "53e0cf94-c68a-4fa0-9849-9505a66bcce6"
            },
            "source": [
                "## Inconsistent Data\n",
                "\n",
                "Check for inconsistent data and address any that arises. As always, use code comments to illustrate your thought process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "azdata_cell_guid": "e9de6624-812a-43f8-8e20-93b4a49b091f"
            },
            "outputs": [],
            "source": [
                "# I noticed that some country names are written differently, like 'U.S.A.' and 'USA'. \n",
                "# I'm creating a small function to fix these so they all group together properly.\n",
                "\n",
                "def fix_countries(name):\n",
                "    name = str(name).strip()\n",
                "    if name == 'U.S.A.' or name == 'USA':\n",
                "        return 'United States'\n",
                "    if name == 'U.K.':\n",
                "        return 'United Kingdom'\n",
                "    if name == 'Domincan Republic': # Fixing a typo I spotted in the original file\n",
                "        return 'Dominican Republic'\n",
                "    return name\n",
                "\n",
                "# Applying the fix to both location columns to keep it clean.\n",
                "df['company_location'] = df['company_location'].apply(fix_countries)\n",
                "df['broad_bean_origin'] = df['broad_bean_origin'].apply(fix_countries)\n",
                "\n",
                "# Finally, I'm saving this clean version as a new CSV for my next assignments.\n",
                "df.to_csv('chocolate_bars_cleaned.csv', index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "dedc0bfe-17d0-40b2-914f-2ddb54f9ce0d"
            },
            "source": [
                "## Summarize Your Results\n",
                "\n",
                "Make note of your answers to the following questions.\n",
                "\n",
                "1. Did you find all four types of dirty data in your dataset?\n",
                "\n",
                "Yes, I found most of them. I had missing data in the bean type columns (hidden as blanks), inconsistent country names like \"U.S.A\" vs \"USA,\" and some irregular ratings that counted as statistical outliers. I checked for unnecessary data (duplicates), and while there weren't many, it was important to verify.\n",
                "\n",
                "2. Did the process of cleaning your data give you new insights into your dataset?\n",
                "\n",
                "Definitely. I realized that \"missing data\" isn't always a null value; sometimes people just leave it blank or hit the space bar. It also showed me just how global the chocolate industry is when I had to standardize all the different country names.\n",
                "\n",
                "3. Is there anything you would like to make note of when it comes to manipulating the data and making visualizations?\n",
                "\n",
                "Converting the cocoa percentage from a string to a float was the most important step. In my last checkpoint, I couldn't do any real math with that column, but now that it's a number, I can actually run statistics and see the trends clearly in my charts."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
